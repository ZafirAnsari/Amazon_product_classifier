Important information:

word2vec_models_analysis.ipynb: word2vec implementation on different models 
				this file also uses stored word2vec embeddings that are saved
				in the folder word2vec_test1
word2vec_models_analysis.ipynb: contains analysis code as well (bots etc (see ppt slide 2))

word2vec_notebook.ipynb (Kaggle): contains the code for making word2vec embeddings and find most
				similar words

best_features*.ipynb (kaggle): contain bayesian vs gridsearchcv comparison

train_test1_test2_combined.ipynb (colab): contains all datasets combined to give a prediction

mode_main_ML*: contains the results of training and testing individually, rather than grouping


deep_learning_methods* (colab) : contain the custom bert model, the saved model can be accessed from the file below
 https://drive.google.com/file/d/1wO44pgTdMmXR2tZtnR83-SJPzYQnsY26/view?usp=sharing

copy of deep_learning*: (colab) contains the base bert code


references and inspiration:
chatgpt (helped with bert, and learning how the model works)
https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/
https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f
https://www.analyticsvidhya.com/blog/2021/06/amazon-product-review-sentiment-analysis-using-bert/
some others generally for kmeans, wordcloud

python 3.10.10

Please note to run the (kaggle) and (colab) files in the respective notebooks.
